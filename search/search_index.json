{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Database Controller Docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The motiviation for this project was to support dynamic creation of database resource and minimal downtime of services during password rotation or DB instance migration.</p> <p>This project implements a database controller. It introduces a CRD that allows clients to create a database claim. That database claim will create a database in an existing postgres server, or it could create a cloud claim that will attach a database on demand. Additionally it will create user/password to the database and rotate them. The database controller will also be used as a proxy driver for connecting to databases.</p>"},{"location":"#assumptions","title":"Assumptions","text":"<p>TBD</p>"},{"location":"#operation","title":"Operation","text":"<p>The Dynamic Database Connection Reconfiguration (DDCR) will consist of three  components that will comprise the database claim pattern implemented by this project. The first will be a db-controller that is responsible for managing the connection  information and associated database.  An application will create its intention  to have a database by adding a  DatabaseClaim Custom Resource (DBClaim). The second is a change to the service deployment that defines a Database Claim.  This claim will provision a k8s secret that replaces the static credentials created by deployment or dynamically retrieved from secret stores by operators like SpaceController.  The third is a database driver that will be able to read the database connection information from the secrets files in the pod volume and construct the  connection string required for connecting through the real database driver. The driver will also be able to detect when the connection information  has changed and gracefully close any open database connections. The closing of the connections will effectively drain the connection  pool as well as trigger a reconnect with the new connection string information by the client service.</p>"},{"location":"#requirements","title":"Requirements","text":"Requirements Description DDCR Establish a pattern (and possible code) for apps to reload connection information from a file DDCR Establish a pattern for mounting a secret that contains connection information in a secret DDCR Support dynamically changing the connection information DDCR Support labels for sharing of dynamically created databases DDCR Modify at least one Atlas app to show as an example of implementing this pattern Cloud Provider Tagging Cloud Resource must be tagged so that then can following organization guidelines. DB Instance Migration CR Change create new database instance and migrate data and delete infrastructure CR DB Instance Migration Migration should with Canary pattern once tests pass all traffic to the new database"},{"location":"#demo","title":"Demo","text":""},{"location":"#configuration","title":"Configuration","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%%  flowchart TB subgraph shared app1--&gt;claim1 app2--&gt;claim2 end subgraph dedicated app3--&gt;claim3 end  claim1 --&gt; rdsinstance1[(RDS Instance 1)] claim2 --&gt; rdsinstance1 claim3 --&gt; rdsinstance2[(RDS Instance 2)]"},{"location":"#policy-demo","title":"Policy Demo","text":"%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%%  flowchart TB subgraph shared policy reclaim1[shared ReclaimPolicy] --&gt; shared[delete] end subgraph default policy reclaim2[default ReclaimPolicy] --&gt; dedicated[retain] end  reclaim1 --&gt; rdsinstance1[(RDS Instance 1)] reclaim2 --&gt; rdsinstance2[(RDS Instance 2)]"},{"location":"#service-architecture","title":"Service Architecture","text":"<p>The first component is a db-controller service that will be responsible  for reading DBClaims and then provisioning the database and user(s)  for that database. The service will also publish this information  into kubernetes secrets, so they can propagate to the other components. The db-controller is also responsible for rotating the user password  based on the password related config values, and updating the user  password in the associated database and kubernetes secrets store.</p> <p>The db-controller will support config values and master  DB instance connection information, defined in a configMap,  as well as the references to the kubernetes basic auth secrets  that contain the root password for each database instance. At startup of the db-controller, the db-controller will scan  any existing DBClaims in its datastore and use the ConnectionInfoUpdatedAt  timestamp as the starting point for the passwordRotation timers.</p> <p>The second component is to modify the pod definition of the application  service to support secrets in files.  In this model, each key within  the secret will be mapped into a separate file in the mountPath of the volume</p> <p>The third component is a database proxy package for the application service. This proxy  will be used in place of the existing database driver. It is configured to register  the real driver for the database type,so that it can forward all the calls to this  driver. The proxy driver will also be configured with a strategy for where it  can find and monitor the secrets associated with the connection string. When the  proxy detects that any of the connection string information has changed  it can close any open connections that the application has with the real database  driver. When the proxy closes the connections, it will trigger the connection  pool to flush any bad connections, and allow the client application to reconnect. The  new database connections for the application will then transparently use the  new connection string information provided by the proxy.</p>   sequenceDiagram     autonumber     CD -&gt;&gt; K8S: apply manifest     K8S -&gt;&gt; CD: ok     rect rgb(0, 255, 0, .3)         loop             K8S-&gt;&gt;db-controller: new DBClaim             rect rgba(0, 0, 255, .2)                 Note over db-controller: Check for Dynamic Database Creation                  db-controller-&gt;&gt;db-controller: Instance exists?                 db-controller-&gt;&gt;infra-operator: No -&gt; CloudDatabaseClaim                 loop Wait for Database Instance                     infra-operator-&gt;&gt;infra-operator: provision instance                 end                 loop Wait for Database Connection Secret                     db-controller-&gt;&gt;db-controller: Connection Secret exists?                 end             end             db-controller-&gt;&gt;RDS: DB exists?             RDS-&gt;&gt;db-controller: no             db-controller-&gt;&gt;RDS: create DB             RDS-&gt;&gt;db-controller: ok             db-controller-&gt;&gt;RDS: create user &amp; set permissions             RDS-&gt;&gt;db-controller: ok             db-controller-&gt;&gt;K8S: create secret with connection string             K8S-&gt;&gt;db-controller: ok         end     end     rect rgb(0, 255, 0, .3)         loop             Kubelet-&gt;&gt;K8S: get secrets             K8S-&gt;&gt;Kubelet: secrets             Kubelet-&gt;&gt;tempfs: store             tempfs-&gt;&gt;Kubelet: ok         end     end     rect rgb(0, 255, 0, .3)         loop             App-&gt;&gt;tempfs: read             tempfs-&gt;&gt;App: ok         end     end"},{"location":"#data-model","title":"Data Model","text":"<p>The database proxy will include a DatabaseClaim Custom Resource. In the first version of the application the DatabaseClaim contained information related  to a specific instance of a database. In the next major release the DatabaseClaim has information to help an infrastructure operator select a database for the client application. This pattern will allow us to make the database selection be pre-provisioned or dynamic on demand. It will also allow the database selction to be multi-cloud e.g. AWS or Azure.</p> <p>DatabaseClaim will  include properties such as the name, appID, Type, etc. During startup,  the db-controller will read the set of CR\u2019s describing the DatabaseClaim and store  them in-memory. In the case of dynamic database provisioning a CR is created to request infrastructure operator to create the database instance. db-controller will check the CR satus and to make sure that the database is provisioned and information about it is available in a secret refrenced by the CR.</p> <p>The db-controller will listen for any changes to the DatabaseClaim CR\u2019s and  remain in-sync with the K8 definitions. In the case of dynamic database provisioning a change will cause a new infrastructure CR to be created, when the new database instance is created we should migrate all password, data and connection to it and delete the old infrastructure CR. It will be infrastructure operator concern if the cloud database instances are deleted immediately or there is a higher level operations workflow to reclaim them. We can break the database migration into phases and deliver just new infrastructure CR on change, this might be fine for development environments and work on the full feature for production. There are also advanced use case like  canary where where an new release of the application creates the new database, tests the migration and once everything is working all traffic is migrated away from the old database and it can be reclaimed.</p> <p>The following shows the mapping of the DatabaseClaim to a CloudDatabase. The CloudDatabaseClaim could be custom or use a infrastructure provider like crossplane resource composition. The DatabaseClaim could also be mapped to a Cloud provider like AWS ACK, providing multi-cloud support by transforming DatabaseClaim to a specific cloud provider CR. In the case of AWS you would leverage ACK RDS Provider.</p> <p>The ClaimMap is meant to transform between DatabaseClaim and CloudDatabaseClaim. We will not use a resource for the ClaimMap, but will use a naming convention to do the mapping and will use the existing FragmentKey for the map to group a number of DatabaseClaim to share a database instance. If more complex use cases arrive we could implement a resource to provide the mapping.</p> %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% erDiagram     DatabaseClaim ||--o{ ClaimMap : \"\"     DatabaseClaim ||--|| CloudDatabaseClaim : \"\"     CloudDatabaseClaim ||--o{ ClaimMap : \"\"  <p>This the deploying with the CR scheme: Relationship of a DatabaseClaim to a Secret and a Pod: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% stateDiagram   direction LR   Application --&gt; DatabaseClaim : Manifest   DatabaseClaim --&gt; CloudDatabaseClaim : Claim Map   CloudDatabaseClaim --&gt; rdsInstance : create instance  Here is an example of what the CloudDatabaseClaim would look like if we used Crossplane. The compositionRef maps our CloudDatabaseClaim to a cloud specific  database instance defintion that is managed by Crossplane: <pre><code>apiVersion: database.infobloxopen.github.com/v1alpha1\nkind: CloudDatabaseClaim\nmetadata:\nname: db-controller-cloud-claim-database-some-app\nnamespace: db-controller-namespace\nspec:\nparameters:\ntype: postgres\nminStorage: 20GB\ncompositionRef:\nname: cloud.database.infobloxopen.github.com\nwriteConnectionSecretToRef:\nname: db-controller-postgres-con-some-app\n</code></pre></p> <p>In this example db-controller-postgres-con-some-app Secret that is written by the infrastructure operator has the connection string and password information for the  db-controller to manage the instance, similar to the pre-provisioned use case.</p>"},{"location":"#configmap","title":"ConfigMap","text":"<p>The db-controller will consume a configMap that contains global config values. <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: db-controller-config\nnamespace: db-controller-namespace\ndata:\nconfig.yaml: |\npasswordConfig:\npasswordComplexity: enabled\nminPasswordLength: \"15\"\npasswordRotationPeriod: \"60\"\natlas:\nusername: root\nhost: some.service\nport: 5432\nsslMode: disable\npasswordSecretRef: atlas-master-password\npasswordSecretKey: password\natlas.recyclebin:\nusername: root\nhost: some.other.service\nport: 5412\nsslMode: disable\npasswordSecretRef: atlas-recyclebin-password\nathena:\nusername=root\nhost=some.service\nport=5432\nsslMode: disable\npasswordSecretRef=athena-master-password\nathena.hostapp:\nusername=root\nhost=some.service\nport=5432\nsslMode: require\npasswordSecretRef=athena-hostapp-password\n</code></pre></p> <ul> <li>authSource: Determines how database host master password is retrieved. The possible values are \"secret\" and \"aws\". In the case of \"secret\" the value from the passwordSecretRef Secret is used for the password. In the case of \"aws\" the RDS password is retrieved using AWS APIs and db-controller should have IAM credentials to make the necessary calls.</li> <li>region: The region where dynamic database is allocated</li> <li>passwordComplexity: Determines if the password adheres to password complexity rules or not.  Values can be enabled or disable.  When enabled, would require the password to meet specific guidelines for password complexity.  The default value is enabled.  Please see the 3rd party section for a sample package that could be used for this.</li> <li>minPasswordLength: Ensures that the generated password is at least this length.  The value is in the range [15, 99].  The default value is 15.  Upper limit is Postgresql max password length limit.</li> <li> <p>passwordRotationPeriod: Defines the period of time (in minutes) before a password is rotated.  The value can be in the range [60, 1440] minutes.  The default value is 60 minutes.</p> </li> <li> <p>Fragment Keys: This is the label to use for identifying the master connection information to a DB instance</p> </li> <li>Username: The username for the master/root user of the database instance</li> <li>Host: The host name where the database instance is located</li> <li>Port: The port to use for connecting to the host</li> <li>sslMode: Indicates of the connection to the DB instance requires secure connection values \"require\" or \"disable\"</li> <li>passwordSecretRef: The name of the secret that contains the password for this connection</li> <li>passwordSecretKey: Optional value for the key value, default value is \"password\"</li> <li>shape: The optional value of shape, see DatabaseClaim, specified here when defined by FragmentKey</li> <li>minStorageGB: The optional value of minStorageGB, see DatabaseClaim, specified here when defined by FragmentKey</li> <li>engineVersion: The optional version of RDS instance, for now Postgres version, but could be other types</li> <li>deletePolicy: The optional DeletePolicy value for CloudDatabase, default delete, possible values: delete, orphan</li> <li> <p>reclaimPolicy: Used as value for ReclaimPolicy for CloudDatabase, possible values are \"delete\" and \"retain\"</p> </li> <li> <p>defaultMasterPort: Value of MasterPort if not specified in FragmentKey</p> </li> <li>defaultMasterUsername: Value of MasterUsername if not specified in FragmentKey</li> <li>defaultSslMode: Value of sslMode if not specified in FragmentKey</li> <li>defaultShape: Value of Shape if not specified in FragmentKey or DatabaseClaim</li> <li>defaultMinStorageGB: Value of MinStorageGB if not specified in FragmentKey or DatabaseClaim </li> <li>defaultEngineVersion: Value of EngineVersion if not specified in FragmentKey or DatabaseClaim</li> <li>defaultDeletionPolicy: The DeletionPolicy for CloudDatabase, possible values: delete, orphan</li> <li>defaultReclaimPolicy: Used as default value for ReclaimPolicy for CloudDatabase, possible values are \"delete\" and \"retain\"</li> </ul> <p>The configMap and credential secrets must be mounted to volumes within the  pod for the db-controller.  This ensures that when the keys are updated, the  projected keys will also be updated in the pod volume. Below is a sample Pod spec with secret and volumes configured: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: db-controller\nspec:\ncontainers:\n- name: db-controller\nimage: infobloxopen/db-controller-abcdefg123\ncommand: [ \"/main\" ]\nvolumeMounts:\n- name: config-volume\nmountPath: /etc/config\n- name: root-user\nmountPath: \"/etc/credentials\"\nreadOnly: true\nvolumes:\n- name: config-volume\nconfigMap:\nname: db-controller-config\n- name: athena-root-user\nsecret:\nsecretName: athena-hostapp-password\nrestartPolicy: Never\n</code></pre> The root user passwords will be stored in kubernetes secrets and the name  must match the passwordSecretRef value in the configMap. Here is sample Secret that match above example: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: athena-hostapp-password\ntype: Opaque\nstringData:\npassword: t0p-Secr3t!@\n</code></pre></p>"},{"location":"#databaseclaim-custom-resource","title":"DatabaseClaim Custom Resource","text":"<p>The DatabaseClaim custom resource describes the connection information for  a database instance.  It includes the following properties:</p> <p>DatabaseClaim:</p> <ul> <li> <p>spec:</p> <ul> <li>AppID: Application ID used for the application.</li> <li>SecretName: The name of the secret to use for storing the ConnectionInfo.  Must follow a naming convention that ensures it is unique.  The SecretName is Namespace scoped.</li> <li>Type: The type of the database instance. E.g. Postgres</li> <li>InstanceLabel: The matching fragment key name of the database instance that will host the database. The fragment keys are defined in the db-controller configMap and describe the connection information for the database instance that this DBClaim is associated with. The longest match will win. For example, the database claim can have a label of athena.hostapp but the only available RDS instances have a label of athena, atlas and northstar. So the controller would match the athena instance. If however an instance label has a label of athena.hostapp, then the hostapp claim would match it exactly.</li> <li>Username: The username that the application will use for accessing the database.</li> <li>DatabaseName: The name of the database instance. </li> <li>DBNameOverride: In most cases the AppID will match the database name. In some cases, however, we will need to provide an optional override.</li> <li>DSNName: The key used for the client dsn connection string in the Secret</li> <li>Host: The optional host name where the database instance is located.  If the value is omitted, then the host value from the matching InstanceLabel will be used.</li> <li>Port: The optional port to use for connecting to the host.  If the value is omitted, then the host value from the matching InstanceLabel will be used.</li> <li>Shape: The optional Shape values are arbitrary and help drive instance selection</li> <li>MinStorageGB: The optional MinStorageGB value requests the minimum database host storage capacity</li> <li>DeletePolicy: The optional DeletePolicy value defines policy, default delete, possible values: delete, recycle</li> </ul> </li> <li> <p>status:</p> <ul> <li>Error: Any errors related to provisioning this claim.</li> <li>DbCreatedAt: Time the database was created.</li> <li>ConnectionInfoUpdatedAt: Time the connection info was updated/created.</li> <li>MatchedLabel: The name of the label that was successfully matched against the fragment key names in the db-controller configMap</li> <li>ConnectionInfo[] (data in connection info is projected into a secret)<ul> <li>Username: The username that the application will use for accessing the database. </li> <li>Password: The password associated with the Username.</li> <li>Host: The host name where the database instance is located.</li> <li>Port: The port to use for connecting to the host.</li> <li>DatabaseName: The name of the database instance.</li> <li>UserUpdatedAt: Time that this user connection information was last updated</li> </ul> </li> </ul> </li> </ul>"},{"location":"#secrets","title":"Secrets","text":"<p>During the processing of each DatabaseClaim, the db-controller will generate the  connection info and also create a secret with the relevant information. The secret  will have the same name as the DatabaseClaim, and contain keys that match the  values of the properties under the DatabaseClaim status.connectionInfo property.</p> <p>The keys in the secret are shown below: * dsn : postgres dsn string value specified by DatabaseClaim * \"uri_\" + dsn : url path value is \"uri_\" prefix added to dsn * \"hostname\" : postgres host of dsn * \"port\" : port used for connecting to the database * \"database\" : postgres database created on host * \"username\" : username to access the database * \"password\" : password to access the database * \"sslmode\" : SSL Mode value as specified by dsn spec</p>"},{"location":"#using-secrets-as-files","title":"Using Secrets as Files","text":"<p>Modify the Pod definition, for the service that you will add the proxy package, to  add a volume under .spec.volumes[]. Name the volume anything, and have  a .spec.volumes[].secret.secretName field equal to the name of the Secret  object, which in this case is also the name of the DatabaseClaim.</p> <p>Add a .spec.containers[].volumeMounts[] to each container that needs  the secret. Specify .spec.containers[].volumeMounts[].readOnly = true  and .spec.containers[].volumeMounts[].mountPath to an unused directory  name where you would like the secrets to appear.</p> <p>Modify your image or command line so that the proxy package looks  for files in that directory. Each key in the secret data map becomes the  filename under mountPath.</p>"},{"location":"#example-secret-config","title":"Example Secret Config","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: mypod\nspec:\ncontainers:\n- name: mypod\nimage: redis\nvolumeMounts:\n- name: foo\nmountPath: \"/etc/connection\"\nreadOnly: true\nvolumes:\n- name: foo\nsecret:\nsecretName: mysecret\n</code></pre> <p>Relationship of a DatabaseClaim to a Secret and a Pod: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% stateDiagram   direction LR   Pod --&gt; Secret : references   Secret --&gt; DatabaseClaim : ownedBy </p>"},{"location":"#api","title":"API","text":"<p>N/A</p>"},{"location":"#implementation","title":"Implementation","text":"<p>TODO Document full implementation for now focused on claim pattern updates</p> <p>This implementation will use the  kubebuilder pattern for implementation of the db-controller. This implementation starts with building a project: <pre><code>kubebuilder init --domain atlas.infoblox.com --repo github.com/infobloxopen/db-controller\n</code></pre> This project was layed down by kubebuilder which depend on sigs.k8s.io/controller-runtime and supports an architecture described here. A key part of this architecture is the Reconciler that encapsulates the custom logic that runs when request are forwarded from Kubernetes API Server to this controller. Specifically it the DatabaseClaim resources that is served.</p> <p>There is a go method (class) defined for handling reconcile functions: <pre><code>// DatabaseClaimReconciler reconciles a DatabaseClaim object\ntype DatabaseClaimReconciler struct {\nclient.Client\nLog        logr.Logger\nScheme     *runtime.Scheme\nConfig     *viper.Viper\nMasterAuth *rdsauth.MasterAuth\n}\n</code></pre> Then the reconcile functions that are parts of the method class are defined with this signature: <pre><code>func (r *DatabaseClaimReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {...}\nfunc (r *DatabaseClaimReconciler) updateStatus(ctx context.Context, dbClaim *persistancev1.DatabaseClaim) (ctrl.Result, error) {...}\n</code></pre></p> <p>The following is the high level view of the reconcile logic: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% graph TB A((Client)) --o B B[K8S API Server] --&gt; C[db-controller] C--&gt; D[Reconciler] D--&gt; E{claim valid?} E -- No --&gt; C E -- Yes --&gt;F{Delete?} F -- No --&gt; G[UpdateStatus] F -- Yes --&gt; H[Clean Up Resources] H --&gt; C G --&gt; C C --&gt;B B --&gt; A </p> <p>The DatabaseClaim Status is where keep the state used by the reconciler. The two parts of the Status that are important are:</p> <ul> <li>MatchedLabel: The name of the label that was successfully matched against the fragment key names in the db-controller configMap</li> <li>ConnectionInfo[]: connection info about the database that is projected into a client accessible secret</li> </ul> <p>There is also a fragmentKey that is the same as the MatchedLabel, but not sure there are two items versus one TBD.</p> <p>In this part we will look at the UpdateStatus function sequence and the proposed changes to support dynamic database creation while leaving much of the working db-controller to interoperate:</p>   sequenceDiagram     UpdateStatus-&gt;&gt;matchInstanceLabel: Spec.InstanceLabel     matchInstanceLabel-&gt;&gt;Status: Status.MatchedLabel     UpdateStatus-&gt;&gt;getClient: Status.MatchedLabel       rect rgba(255, 0, 255, .2)          par New Dynamic Database Binding           getClient-&gt;&gt;getHost: dbClaim           getHost-&gt;&gt;getHost: Spec.Host &amp;&amp; Config::Status.MatchedLabel.Host == \"\" ?           getHost-&gt;&gt;getHost: Yes           Status-&gt;&gt;getHost: Status.CloudDatabase.Host == \"\" ?           getHost-&gt;&gt;getHost: Yes                   getHost-&gt;&gt;createCloudDatabase: dbClaim           createCloudDatabase-&gt;&gt;createCloudDatabase: Wait for resource ready           createCloudDatabase-&gt;&gt;getHost: CloudDatabase           getHost-&gt;&gt;Status: Status.CloudDatabase.{Name, Host, Port, User, Password}           getHost-&gt;&gt;getClient: {Host, Port, User, Password}         end       end       rect rgba(255, 255, 0, .2)          par Existing Static Database Binding           getHost-&gt;&gt;getHost: Spec.Host &amp;&amp; Config::Status.MatchedLabel.Host == \"\"           getHost-&gt;&gt;getHost: No           getProvisionedHost-&gt;&gt;getHost: {Host, Port, User, Password}           getHost-&gt;&gt;getClient: {Host, Port, User, Password}         end     end        getClient-&gt;&gt;Status: Status.ConnectionInfo.Host     getClient-&gt;&gt;Status: Status.ConnectionInfo.Port     getClient-&gt;&gt;Status: Status.ConnectionInfo.SSLMode     getClient-&gt;&gt;Status: Status.ConnectionInfo.ConnectionInfoUpdatedAt     getClient-&gt;&gt;DBClientFactory: host, port, user, password, sslmode     DBClientFactory-&gt;&gt;getClient: dbClient     getClient-&gt;&gt;UpdateStatus: dbClient     UpdateStatus-&gt;&gt;GetDBName: dbClaim     GetDBName-&gt;&gt;UpdateStatus: Spec.DatabaseName or Spec.DBNameOverride"},{"location":"#lifecycle","title":"Lifecycle","text":"<p>TODO - Document the full lifecycle not just for Dynamic Host</p> <p>The dynamic host allocation will have the following lifecycle:</p> <p>The create and update lifecycles are fairly simple: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% stateDiagram-v2 CDH: Create Dynamic Host CDU: Update Dynamic Host  [*] --&gt; DbClaim state DbClaim {     Create --&gt; CDH     --     Update --&gt; CDU } </p> <p>The delete lifecycle is more complex shown below:</p> %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% stateDiagram-v2 DCD: Delete CloudDatabase DDC: Delete DatabaseClaim  [*] --&gt; DbClaim state DbClaim {     Delete --&gt; Finalizer     state Finalizer {         state check_host &lt;&gt;         state check_label &lt;&gt;         [*] --&gt; check_host         check_host --&gt; check_label: if Claim.Host == \"\"         check_host --&gt; [*] : if Claim.Host != \"\"         check_label --&gt; DCD: if ReclaimPolicy == Delete &amp;&amp; DbClaims using CloudDB == 1         check_label --&gt; [*] : if ReclaimPolicy == Retain         DCD --&gt; [*]     }     Finalizer --&gt; DDC     DDC --&gt; [*] }  <p>The sharing of database by multiple applicaitons (Claims) by Crossplane is an open issue. The db-controller creates the CloudDatabase Claim and the associated connection secret is created in the db-controller namespace. We can allow sharing of the database using the FragmentKey structure by mapping to the FragmentKKey using DatabaseClaim InstanceLabel property.</p> <p>Crossplane started  with the intent of using a similar pattern used by pv and pvc in Kubernetes. The ReclaimPolicy had a lot off issues and it got backed out and it got renamed to DeletionPolicy on just the CloudDatabase resource.</p> <p>DatabaseClaim resources are namespaced, but the CloudDatabase resources are dynamically provisioned as cluster scoped. A namespaced resource cannot, by design, own a cluster scoped resource. This could be an issue in how we manage lifecycle, see kubernetes and crossplane references.</p> <p>Given the Crossplane experience we will retain the FragmentKey allocation CloudDatabase cluster resource so that it can be shared. The deletion of ClaimDatabase will remove the CloudDatabase directly allocated directly by its claim, similar behavior to Crossplane composite behavior </p> <p>We need some more research PoC to figure out if we can use  Kubernetes Finalizers on DatabaseClaim to honor a ReclaimPolicy with values of delete and retain. Then have ReclaimPolicy delete a dynamically allocated database that is no longer shared. The reason for the  deprecation of ReclaimPolicy from CrossPlane needs to be examined in the light of our requirements and why Crossplane does not offer a  Shared Database Pattern. A argument made here  suggest that in most cases sharing infrastructure within namespace  boundaries with a  single claim is valid, as namespace == application  team boundary. Does forcing applciations to share namespace for database sharing cause any issues, e.g. RBAC?</p> <p>The initial PoC will manage ReclaimPolicy from the db-controller configuration using the InstanceLabels that enable sharing. There will be a DatabaseClaim finalizer that will run  and if there are no more claims against the InstanceLabel then the dynamic database is reclaimed. There will also be a global and reclaim policy per InstanceLabel/FragmentKey. This will not be exposed to the Applications e.g. ReclaimPolicy  in the DatabaseClaim. I don't think we need this complexity but could be added if there is a need.</p> <p>The Crossplane Infrastructure Operator, has a resource DeletionPolicy which specifies what will happen  to the underlying external cloud resource when this managed resource is  deleted - either \"Delete\" or \"Orphan\" the external resource. This will be managed by defaultDeletionPolicy and deletionPolicy on each FragmentKey.</p> <p>Another features we have is to share resources like RDS among different clusters that are deployed. We don't have a design yet on how to best support this requirement. Crossplane Infrastructure Operator supports multiple clusters, using an admin cluster and then resources like CloudDatabases are pushed to the managed clusters, Crossplane workload reference. Crossplane also have an observerable pattern to deal with this coupling between clusters trying to share resources.</p>"},{"location":"#deployment","title":"Deployment","text":"<p>When API is updated need to update the crd definition and helm chart that updates it. The following make target is setup to do this: <pre><code>make update_crds\n</code></pre></p>"},{"location":"#authentication","title":"Authentication","text":""},{"location":"#authorization","title":"Authorization","text":"<p>This db-controller service will require access to credentials that have permissions to provision the database as well as access to the master user account to manage users for the target database.</p>"},{"location":"#ingress","title":"Ingress","text":"<p>N/A</p>"},{"location":"#audit","title":"Audit","text":"<p>N/A</p>"},{"location":"#logging","title":"Logging","text":"<p>TBD</p>"},{"location":"#alerting","title":"Alerting","text":"<p>Prometheus alerts based on errors from the db-controller.</p>"},{"location":"#performance","title":"Performance","text":"<p>N/A</p>"},{"location":"#security","title":"Security","text":"<p>TBD</p>"},{"location":"#third-party-software","title":"Third-Party Software","text":"Name/Version License Description/Reason for usage fsnotify BSD-3-Clause Golang cross platform file system notifications go-password MIT License Generation of random passwords with provided requirements as described by  AgileBits"},{"location":"#metricstelemetry","title":"Metrics/Telemetry","text":"<ul> <li>The following metrics should be exposed:</li> <li>Total database users created</li> <li>Time to create a new database user</li> <li>Total database provisioning errors</li> <li>Total database users created with errors</li> <li>Total passwords rotated</li> <li>Time to rotate user password</li> <li>Total password rotated with error</li> <li>Total DBClaim load errors</li> <li>Total DBClaims loaded</li> <li>Time to load a DBClaim</li> </ul>"},{"location":"#rate-limits","title":"Rate Limits","text":"<p>N/A</p>"},{"location":"#uxui","title":"UX/UI","text":"<p>N/A</p>"},{"location":"#high-availabilityresiliency","title":"High Availability/Resiliency","text":"<p>TBD</p>"},{"location":"#scalability","title":"Scalability","text":"<p>The db-controller service needs to be able to scale out based on demand.</p>"},{"location":"#backup","title":"Backup","text":"<p>N/A</p>"},{"location":"#disaster-recovery","title":"Disaster Recovery","text":"<p>TBD</p>"},{"location":"#data-retention","title":"Data Retention","text":"<p>N/A</p>"},{"location":"#testing","title":"Testing","text":""},{"location":"#test-plan","title":"Test Plan","text":"<p>TBD</p>"},{"location":"#test-matrix","title":"Test Matrix","text":"<p>TBD</p>"},{"location":"#migration","title":"Migration","text":"<p>N/A</p>"},{"location":"#deployment-architecture","title":"Deployment Architecture","text":"<p>TBD</p>"},{"location":"#upgrades","title":"Upgrades","text":"<p>N/A</p>"},{"location":"#service-dependencies","title":"Service Dependencies","text":"<p>This service depends on these services: * kube-api-server * SpaceController or similar operator: updates the RDS root passwords that the db-controller can use</p> <p>These services depend on this service: N/A</p>"},{"location":"#service-killers","title":"Service Killers","text":"Service Name Impact RDS Can't create new databases, can't rotate passwords. can't process new claims. kube-apiserver full down."},{"location":"#data-analytics","title":"Data Analytics","text":"<p>N/A</p>"},{"location":"#references","title":"References","text":"<p>Hotload database driver</p>"}]}